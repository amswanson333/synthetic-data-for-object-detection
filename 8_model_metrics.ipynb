{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c4ebde",
   "metadata": {},
   "source": [
    "# ðŸ“ˆ Model Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae647c1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf52dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72f84c",
   "metadata": {},
   "source": [
    "## 1. File Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9435a12a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d3a8b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the staging directory if it doesn't exist\n",
    "os.makedirs(os.path.join(\"data\", \"staging\"), exist_ok=True)\n",
    "# Create the test folder in the staging directory if it doesn't exist\n",
    "os.makedirs(os.path.join(\"data\", \"staging\", \"test\"), exist_ok=True)\n",
    "# Create train and val folders (they won't be used but need to exist for the YOLO model functions)\n",
    "os.makedirs(os.path.join(\"data\", \"staging\", \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(\"data\", \"staging\", \"val\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "218f4488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the YOLO yaml file\n",
    "yaml_path = os.path.join(\"data\", \"staging\", \"evaluation.yaml\")\n",
    "\n",
    "# YAML content\n",
    "yaml_content = \"\"\"\n",
    "path: data/staging  # dataset root dir (leave empty for HUB)\n",
    "train: train  # train images (relative to 'path')\n",
    "val:   val    # val images (relative to 'path')\n",
    "test:  test   # test images (relative to 'path')\n",
    "\n",
    "names:\n",
    "  0: drone\n",
    "\"\"\"\n",
    "\n",
    "# Write the YAML content to the file\n",
    "with open(yaml_path, 'w') as yaml_file:\n",
    "    yaml_file.write(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be377439",
   "metadata": {},
   "source": [
    "**Experimental Design**\n",
    "\n",
    "| **Run** | **Authentic Data** | **3D Model Data** | **Clipart Data** | **Gen AI Data** |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 00  | 100 | 0   | 0   | 0   |\n",
    "| 01  | 26  | 27  | 23  | 24  |\n",
    "| 02  | 0   | 57  | 22  | 21  |\n",
    "| 03  | 0   | 27  | 73  | 0   |\n",
    "| 04  | 27  | 0   | 0   | 73  |\n",
    "| 05  | 27  | 33  | 40  | 0   |\n",
    "| 06  | 38  | 0   | 33  | 29  |\n",
    "| 07  | 38  | 33  | 29  | 0   |\n",
    "| 08  | 0   | 0   | 76  | 24  |\n",
    "| 09  | 0   | 24  | 52  | 24  |\n",
    "| 10  | 0   | 100 | 0   | 0   |\n",
    "| 11  | 27  | 73  | 0   | 0   |\n",
    "| 12  | 0   | 0   | 30  | 70  |\n",
    "| 13  | 29  | 0   | 36  | 35  |\n",
    "| 14  | 70  | 30  | 0   | 0   |\n",
    "| 15  | 72  | 0   | 0   | 28  |\n",
    "| 16  | 0   | 0   | 100 | 0   |\n",
    "| 17  | 26  | 35  | 0   | 39  |\n",
    "| 18  | 0   | 27  | 0   | 73  |\n",
    "| 19  | 0   | 70  | 30  | 0   |\n",
    "| 20  | 40  | 30  | 0   | 30  |\n",
    "| 21  | 27  | 0   | 73  | 0   |\n",
    "| 22  | 70  | 0   | 30  | 0   |\n",
    "| 23  | 0   | 0   | 0   | 100 |\n",
    "| 24  | 0   | 26  | 27  | 47  |\n",
    "| 25  | 0   | 72  | 0   | 28  |\n",
    "| 26* | 0   | 0   | 50  | 50  |\n",
    "| 27* | 0   | 50  | 0   | 50  |\n",
    "| 28* | 0   | 50  | 50  | 0   |\n",
    "| 29* | 0   | 33  | 33  | 33  |\n",
    "\n",
    "`* denotes extra runs that were added`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca87717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model suffixes\n",
    "# It would be better to do this by searching the directory, but this is quicker for now.\n",
    "model_00 = \"baseline\"\n",
    "\n",
    "model_01 = \"26-27-23-24\"\n",
    "model_02 = \"0-56-22-21\"\n",
    "model_03 = \"0-27-73-0\"\n",
    "model_04 = \"27-0-0-73\"\n",
    "model_05 = \"27-33-40-0\"\n",
    "model_06 = \"38-0-33-28\"\n",
    "model_07 = \"38-33-28-0\"\n",
    "model_08 = \"0-0-76-24\"\n",
    "model_09 = \"0-24-52-24\"\n",
    "model_10 = \"0-100-0-0\"\n",
    "model_11 = \"27-73-0-0\"\n",
    "model_12 = \"0-0-30-70\"\n",
    "model_13 = \"28-0-36-35\"\n",
    "model_14 = \"70-30-0-0\"\n",
    "model_15 = \"72-0-0-28\"\n",
    "model_16 = \"0-0-100-0\"\n",
    "model_17 = \"26-35-0-39\"\n",
    "model_18 = \"0-27-0-73\"\n",
    "model_19 = \"0-70-30-0\"\n",
    "model_20 = \"40-30-0-30\"\n",
    "model_21 = \"27-0-73-0\"\n",
    "model_22 = \"70-0-30-0\"\n",
    "model_23 = \"0-0-0-100\"\n",
    "model_24 = \"0-26-27-47\"\n",
    "model_25 = \"0-72-0-28\"\n",
    "model_26 = \"0-0-50-50\"\n",
    "model_27 = \"0-50-0-50\"\n",
    "model_28 = \"0-50-50-0\"\n",
    "model_29 = \"0-33-33-33\"\n",
    "\n",
    "# Combine all model suffixes into a list\n",
    "model_suffixes = [model_00,\n",
    "                  model_01,\n",
    "                  model_02,\n",
    "                  model_03,\n",
    "                  model_04,\n",
    "                  model_05,\n",
    "                  model_06,\n",
    "                  model_07,\n",
    "                  model_08,\n",
    "                  model_09,\n",
    "                  model_10,\n",
    "                  model_11,\n",
    "                  model_12,\n",
    "                  model_13,\n",
    "                  model_14,\n",
    "                  model_15,\n",
    "                  model_16,\n",
    "                  model_17,\n",
    "                  model_18,\n",
    "                  model_19,\n",
    "                  model_20,\n",
    "                  model_21,\n",
    "                  model_22,\n",
    "                  model_23,\n",
    "                  model_24,\n",
    "                  model_25,\n",
    "                  model_26,\n",
    "                  model_27,\n",
    "                  model_28,\n",
    "                  model_29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "595b0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "for suffix in model_suffixes:\n",
    "    dir_path = os.path.join(\"model\", f\"model_{suffix}\")\n",
    "    os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d43dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of important file paths for each model\n",
    "model_paths = {\n",
    "    suffix: {\n",
    "        \"weights\": os.path.join(\"model\", f\"model_{suffix}\", \"weights\", \"best.pt\"),\n",
    "        \"train_df\": os.path.join(\"model\", f\"model_{suffix}\", f\"train_data_{suffix}.csv\"),\n",
    "        \"val_df\": os.path.join(\"model\", f\"model_{suffix}\", f\"val_data_{suffix}.csv\"),\n",
    "        \"results_df\": os.path.join(\"model\", f\"model_{suffix}\", \"results\", f\"results_{suffix}.csv\"),\n",
    "        }\n",
    "    for suffix in model_suffixes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8774c93",
   "metadata": {},
   "source": [
    "## 2. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39003e03",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7eb593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "# Cleanup staging directory\n",
    "print(\"Cleaning up staging directory...\")\n",
    "utils.files.cleanup_staging()\n",
    "print(\"Cleaning complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeaf04d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of testing images\n",
    "test_images = utils.files.get_image_list(os.path.join(utils.PROCESSED_DATA_DIR, \"testing\", \"test\"))\n",
    "print(f\"Number of test images: {len(test_images)}\")\n",
    "print(\"Example image path: \", test_images[0] if test_images else \"No images found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db5907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of destination paths in staging directory\n",
    "destination_path = os.path.join(\"data\", \"staging\", \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined results dataframe to store all results\n",
    "combined_results_df = pd.DataFrame(columns=[\"data_blend\", \"test_map50\", \"test_map50-95\"])\n",
    "\n",
    "# Loop through the different models\n",
    "for suffix in model_suffixes:\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Processing model_{suffix}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create a results dataframe to store the results\n",
    "    metrics_df = pd.DataFrame(columns=[\"data_blend\", \"map50\", \"map50-95\"])\n",
    "    \n",
    "    # Move data to the staging directory\n",
    "    print(\"Copying data to staging directory...\")\n",
    "    print(f\"\\tReading training data from: {model_paths[suffix]['train_df']}\")\n",
    "    print(f\"\\tReading validation data from: {model_paths[suffix]['val_df']}\")\n",
    "    train_df = pd.read_csv(model_paths[suffix][\"train_df\"])\n",
    "    val_df = pd.read_csv(model_paths[suffix][\"val_df\"])\n",
    "    utils.files.copy_to_staging(train_df, stage=\"test\")\n",
    "    utils.files.copy_to_staging(val_df, stage=\"test\")\n",
    "    print(\"Data copy complete.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Do stuff\n",
    "    # Load the model\n",
    "    print(f\"Loading baseline model from: {baseline_model}...\")\n",
    "    model = YOLO(baseline_model)\n",
    "    print(\"Model loaded.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Evaluate the model on the data mix\n",
    "    # Assess the recognizability metric (mAP50 and mAP50-95)\n",
    "    print(\"Evaluating model recognizability...\")\n",
    "    print(\"Running data blend through baseline model for evaluation...\")\n",
    "    recog_results = model.val(data=yaml_path, split=\"test\")\n",
    "    model_map50 = recog_results.box.map50\n",
    "    model_map50_95 = recog_results.box.map\n",
    "    print(\"Evaluation complete.\")\n",
    "    print(f\"mAP50: {model_map50:.4f}\")\n",
    "    print(f\"mAP50-95: {model_map50_95:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Evaluate the diversity metric\n",
    "    print(\"Evaluating model diversity...\")\n",
    "    layer_indicies = [10, 14, 17] # Indices of layers to extract feature embeddings from\n",
    "    blend_images = utils.files.get_image_files(os.path.join(\"data\", \"staging\", \"test\"))\n",
    "    print(f\"Extracting feature embeddings from {len(blend_images)} test images...\")\n",
    "    div_results = []\n",
    "    for img in blend_images:\n",
    "        div_result = model.predict(img, embed=layer_indicies, verbose=False)\n",
    "        div_results.append(div_result[0])\n",
    "    print(\"Stacking feature embeddings into a single tensor...\")\n",
    "    div_tensor = torch.stack(div_results)\n",
    "    print(f\"Feature embeddings tensor shape: {div_tensor.shape}\")\n",
    "    print(\"Calculating total variance across feature embeddings...\")\n",
    "    cov_matrix = torch.cov(div_tensor.view(div_tensor.size(0), -1).T)\n",
    "    diversity_metric = torch.trace(cov_matrix).item()\n",
    "    print(f\"Total Variance (Diversity Metric): {diversity_metric:.4f}\")\n",
    "    print(\"Diversity evaluation complete.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Append results to the dataframe\n",
    "    print(\"Saving results...\")\n",
    "    print(f\"Ensuring file path exists: {model_paths[suffix]['results_df']}\")\n",
    "    os.makedirs(os.path.dirname(model_paths[suffix]['results_df']), exist_ok=True)\n",
    "    metrics = pd.Series({\n",
    "        \"data_blend\": suffix,\n",
    "        \"map50\": model_map50,\n",
    "        \"map50-95\": model_map50_95,\n",
    "        \"total_variance\": diversity_metric\n",
    "        })\n",
    "    metrics_df = pd.concat([metrics_df, metrics.to_frame().T], ignore_index=True)\n",
    "    combined_results_df = pd.concat([combined_results_df, metrics.to_frame().T], ignore_index=True)\n",
    "    # Save the results dataframe\n",
    "    metrics_df.to_csv(model_paths[suffix][\"results_df\"], index=False)\n",
    "    print(\"Results saved.\")\n",
    "    \n",
    "print(\"=\" * 50)\n",
    "print(\"All models processed.\")\n",
    "\n",
    "# Save the combined results dataframe\n",
    "combined_results_path = os.path.join(\"reports\", \"diversity_recognizability_results.csv\")\n",
    "combined_results_df.to_csv(combined_results_path, index=False)\n",
    "print(\"Combined results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sydrone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
