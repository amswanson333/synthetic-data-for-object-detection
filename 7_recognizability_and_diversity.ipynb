{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7111cc1d",
   "metadata": {},
   "source": [
    "# üîé Recognizability and üéÜ Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bbbfc",
   "metadata": {},
   "source": [
    "The ideas of recognizability and diversity of a data set introducted by Boutin et al. (2022) are used to evaluate the ability of a generative model to create useful data. The recognizability metric is easiest to understand as simply how easy (or difficult) it is for the data to be classified. Therefore, in the case of the drone data it is just a measure of how easily the drone objects can be identified within the images. Diversity is best thought of as the variance of the feature space of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a7a56",
   "metadata": {},
   "source": [
    "`Boutin, V., Singhal, L., Thomas, X., & Serre, T. (2022). Diversity vs. Recognizability: Human-like generalization in one-shot generative models. Advances in Neural Information Processing Systems, 35, 20933-20946.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab3ea6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901175c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63805d88",
   "metadata": {},
   "source": [
    "## 1. File Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba3641",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb14c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the staging directory if it doesn't exist\n",
    "os.makedirs(os.path.join(\"data\", \"staging\"), exist_ok=True)\n",
    "# Create the test folder in the staging directory if it doesn't exist\n",
    "os.makedirs(os.path.join(\"data\", \"staging\", \"test\"), exist_ok=True)\n",
    "# Create train and val folders (they won't be used but need to exist for the YOLO model functions)\n",
    "os.makedirs(os.path.join(\"data\", \"staging\", \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(\"data\", \"staging\", \"val\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5551278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the YOLO yaml file\n",
    "yaml_path = os.path.join(\"data\", \"staging\", \"evaluation.yaml\")\n",
    "\n",
    "# YAML content\n",
    "yaml_content = \"\"\"\n",
    "path: data/staging  # dataset root dir (leave empty for HUB)\n",
    "train: train  # train images (relative to 'path')\n",
    "val:   val    # val images (relative to 'path')\n",
    "test:  test   # test images (relative to 'path')\n",
    "\n",
    "names:\n",
    "  0: drone\n",
    "\"\"\"\n",
    "\n",
    "# Write the YAML content to the file\n",
    "with open(yaml_path, 'w') as yaml_file:\n",
    "    yaml_file.write(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab8af6",
   "metadata": {},
   "source": [
    "**Experimental Design**\n",
    "\n",
    "| **Run** | **Authentic Data** | **3D Model Data** | **Clipart Data** | **Gen AI Data** |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 00  | 100 | 0   | 0   | 0   |\n",
    "| 01  | 26  | 27  | 23  | 24  |\n",
    "| 02  | 0   | 57  | 22  | 21  |\n",
    "| 03  | 0   | 27  | 73  | 0   |\n",
    "| 04  | 27  | 0   | 0   | 73  |\n",
    "| 05  | 27  | 33  | 40  | 0   |\n",
    "| 06  | 38  | 0   | 33  | 29  |\n",
    "| 07  | 38  | 33  | 29  | 0   |\n",
    "| 08  | 0   | 0   | 76  | 24  |\n",
    "| 09  | 0   | 24  | 52  | 24  |\n",
    "| 10  | 0   | 100 | 0   | 0   |\n",
    "| 11  | 27  | 73  | 0   | 0   |\n",
    "| 12  | 0   | 0   | 30  | 70  |\n",
    "| 13  | 29  | 0   | 36  | 35  |\n",
    "| 14  | 70  | 30  | 0   | 0   |\n",
    "| 15  | 72  | 0   | 0   | 28  |\n",
    "| 16  | 0   | 0   | 100 | 0   |\n",
    "| 17  | 26  | 35  | 0   | 39  |\n",
    "| 18  | 0   | 27  | 0   | 73  |\n",
    "| 19  | 0   | 70  | 30  | 0   |\n",
    "| 20  | 40  | 30  | 0   | 30  |\n",
    "| 21  | 27  | 0   | 73  | 0   |\n",
    "| 22  | 70  | 0   | 30  | 0   |\n",
    "| 23  | 0   | 0   | 0   | 100 |\n",
    "| 24  | 0   | 26  | 27  | 47  |\n",
    "| 25  | 0   | 72  | 0   | 28  |\n",
    "| 26* | 0   | 0   | 50  | 50  |\n",
    "| 27* | 0   | 50  | 0   | 50  |\n",
    "| 28* | 0   | 50  | 50  | 0   |\n",
    "| 29* | 0   | 33  | 33  | 33  |\n",
    "\n",
    "`* denotes extra runs that were added`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af19956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model suffixes\n",
    "# It would be better to do this by searching the directory, but this is quicker for now.\n",
    "model_00 = \"baseline\"\n",
    "\n",
    "model_01 = \"26-27-23-24\"\n",
    "model_02 = \"0-56-22-21\"\n",
    "model_03 = \"0-27-73-0\"\n",
    "model_04 = \"27-0-0-73\"\n",
    "model_05 = \"27-33-40-0\"\n",
    "model_06 = \"38-0-33-28\"\n",
    "model_07 = \"38-33-28-0\"\n",
    "model_08 = \"0-0-76-24\"\n",
    "model_09 = \"0-24-52-24\"\n",
    "model_10 = \"0-100-0-0\"\n",
    "model_11 = \"27-73-0-0\"\n",
    "model_12 = \"0-0-30-70\"\n",
    "model_13 = \"28-0-36-35\"\n",
    "model_14 = \"70-30-0-0\"\n",
    "model_15 = \"72-0-0-28\"\n",
    "model_16 = \"0-0-100-0\"\n",
    "model_17 = \"26-35-0-39\"\n",
    "model_18 = \"0-27-0-73\"\n",
    "model_19 = \"0-70-30-0\"\n",
    "model_20 = \"40-30-0-30\"\n",
    "model_21 = \"27-0-73-0\"\n",
    "model_22 = \"70-0-30-0\"\n",
    "model_23 = \"0-0-0-100\"\n",
    "model_24 = \"0-26-27-47\"\n",
    "model_25 = \"0-72-0-28\"\n",
    "model_26 = \"0-0-50-50\"\n",
    "model_27 = \"0-50-0-50\"\n",
    "model_28 = \"0-50-50-0\"\n",
    "model_29 = \"0-33-33-33\"\n",
    "\n",
    "# Combine all model suffixes into a list\n",
    "model_suffixes = [model_00,\n",
    "                  model_01,\n",
    "                  model_02,\n",
    "                  model_03,\n",
    "                  model_04,\n",
    "                  model_05,\n",
    "                  model_06,\n",
    "                  model_07,\n",
    "                  model_08,\n",
    "                  model_09,\n",
    "                  model_10,\n",
    "                  model_11,\n",
    "                  model_12,\n",
    "                  model_13,\n",
    "                  model_14,\n",
    "                  model_15,\n",
    "                  model_16,\n",
    "                  model_17,\n",
    "                  model_18,\n",
    "                  model_19,\n",
    "                  model_20,\n",
    "                  model_21,\n",
    "                  model_22,\n",
    "                  model_23,\n",
    "                  model_24,\n",
    "                  model_25,\n",
    "                  model_26,\n",
    "                  model_27,\n",
    "                  model_28,\n",
    "                  model_29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a4826e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "for suffix in model_suffixes:\n",
    "    dir_path = os.path.join(\"model\", f\"model_{suffix}\")\n",
    "    os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1e11aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of important file paths for each model\n",
    "model_paths = {\n",
    "    suffix: {\n",
    "        \"weights\": os.path.join(\"model\", f\"model_{suffix}\", \"weights\", \"best.pt\"),\n",
    "        \"train_df\": os.path.join(\"model\", f\"model_{suffix}\", f\"train_data_{suffix}.csv\"),\n",
    "        \"val_df\": os.path.join(\"model\", f\"model_{suffix}\", f\"val_data_{suffix}.csv\"),\n",
    "        \"results_df\": os.path.join(\"model\", f\"model_{suffix}\", \"results\", f\"results_{suffix}.csv\"),\n",
    "        }\n",
    "    for suffix in model_suffixes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e500c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights file for the baseline model for comparison purposes\n",
    "baseline_model = model_paths[model_00][\"weights\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5e0ee",
   "metadata": {},
   "source": [
    "## 2. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f888fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b97fa88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing model_baseline\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_baseline\\train_data_baseline.csv\n",
      "\tReading validation data from: model\\model_baseline\\val_data_baseline.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 147.2127.1 MB/s, size: 1192.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 15263 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 437.1it/s 56.2s<0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.5it/s 2:14<0.2s\n",
      "                   all      24568       9327      0.894      0.797      0.873      0.634\n",
      "Speed: 0.4ms preprocess, 4.0ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val2\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.8732\n",
      "mAP50-95: 0.6341\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 40.2585\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_baseline\\results\\results_baseline.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_26-27-23-24\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_26-27-23-24\\train_data_26-27-23-24.csv\n",
      "\tReading validation data from: model\\model_26-27-23-24\\val_data_26-27-23-24.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 226.4162.2 MB/s, size: 1515.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 9575 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 332.2it/s 1:14<0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.4it/s 2:15<0.1s\n",
      "                   all      24568      22202      0.652      0.199      0.438      0.278\n",
      "Speed: 0.4ms preprocess, 4.3ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val3\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.4377\n",
      "mAP50-95: 0.2783\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 115.9287\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_26-27-23-24\\results\\results_26-27-23-24.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-56-22-21\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-56-22-21\\train_data_0-56-22-21.csv\n",
      "\tReading validation data from: model\\model_0-56-22-21\\val_data_0-56-22-21.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 248.597.1 MB/s, size: 1902.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24567 images, 7347 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24567/24567 272.3it/s 1:30<0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.0it/s 2:19<0.1s\n",
      "                   all      24567      27519      0.482      0.103      0.292      0.167\n",
      "Speed: 0.4ms preprocess, 4.5ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val4\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.2915\n",
      "mAP50-95: 0.1672\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24567 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24567, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 146.7393\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-56-22-21\\results\\results_0-56-22-21.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-27-73-0\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-27-73-0\\train_data_0-27-73-0.csv\n",
      "\tReading validation data from: model\\model_0-27-73-0\\val_data_0-27-73-0.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 261.476.0 MB/s, size: 1498.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 7232 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 238.3it/s 1:43<0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 10.5it/s 2:27<0.2s\n",
      "                   all      24568      27993      0.554      0.138      0.345      0.209\n",
      "Speed: 0.4ms preprocess, 4.7ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val5\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3449\n",
      "mAP50-95: 0.2092\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 152.2923\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-27-73-0\\results\\results_0-27-73-0.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_27-0-0-73\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_27-0-0-73\\train_data_27-0-0-73.csv\n",
      "\tReading validation data from: model\\model_27-0-0-73\\val_data_27-0-0-73.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 144.749.5 MB/s, size: 623.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 10532 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 676.7it/s 36.3s<0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 12.9it/s 1:59<0.1s\n",
      "                   all      24568      19175      0.684      0.238      0.479      0.289\n",
      "Speed: 0.3ms preprocess, 3.7ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val6\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.4785\n",
      "mAP50-95: 0.2887\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 96.3237\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_27-0-0-73\\results\\results_27-0-0-73.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_27-33-40-0\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_27-33-40-0\\train_data_27-33-40-0.csv\n",
      "\tReading validation data from: model\\model_27-33-40-0\\val_data_27-33-40-0.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 259.2102.3 MB/s, size: 1695.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 9276 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 266.9it/s 1:32<0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 10.8it/s 2:22<0.1s\n",
      "                   all      24568      23249      0.641      0.192      0.427      0.281\n",
      "Speed: 0.4ms preprocess, 4.5ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val7\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.4267\n",
      "mAP50-95: 0.2806\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 122.4482\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_27-33-40-0\\results\\results_27-33-40-0.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_38-0-33-28\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_38-0-33-28\\train_data_38-0-33-28.csv\n",
      "\tReading validation data from: model\\model_38-0-33-28\\val_data_38-0-33-28.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 82.653.6 MB/s, size: 556.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 10680 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 371.5it/s 1:06<0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.6it/s 2:13<0.1s\n",
      "                   all      24568      19376      0.714       0.28      0.516      0.337\n",
      "Speed: 0.3ms preprocess, 4.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val8\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.5160\n",
      "mAP50-95: 0.3373\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 101.5971\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_38-0-33-28\\results\\results_38-0-33-28.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_38-33-28-0\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_38-33-28-0\\train_data_38-33-28-0.csv\n",
      "\tReading validation data from: model\\model_38-33-28-0\\val_data_38-33-28-0.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 188.159.5 MB/s, size: 1746.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 10167 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 278.9it/s 1:28<0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.0it/s 2:20<0.1s\n",
      "                   all      24568      21282      0.685      0.228      0.471      0.316\n",
      "Speed: 0.4ms preprocess, 4.4ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val9\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.4707\n",
      "mAP50-95: 0.3164\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 110.5228\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_38-33-28-0\\results\\results_38-33-28-0.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-0-76-24\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-0-76-24\\train_data_0-0-76-24.csv\n",
      "\tReading validation data from: model\\model_0-0-76-24\\val_data_0-0-76-24.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 2.83.8 ms, read: 77.918.5 MB/s, size: 1096.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 7672 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 276.4it/s 1:29<0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 10.9it/s 2:21<0.1s\n",
      "                   all      24568      26459      0.595      0.162       0.38      0.223\n",
      "Speed: 0.4ms preprocess, 4.5ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val10\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3801\n",
      "mAP50-95: 0.2226\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 142.8690\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-0-76-24\\results\\results_0-0-76-24.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-24-52-24\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-24-52-24\\train_data_0-24-52-24.csv\n",
      "\tReading validation data from: model\\model_0-24-52-24\\val_data_0-24-52-24.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 172.384.6 MB/s, size: 929.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24567 images, 7484 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24567/24567 282.6it/s 1:27<0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 10.9it/s 2:21<0.1s\n",
      "                   all      24567      26984      0.562      0.139      0.351      0.204\n",
      "Speed: 0.4ms preprocess, 4.5ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val11\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3511\n",
      "mAP50-95: 0.2045\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24567 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24567, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 146.2762\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-24-52-24\\results\\results_0-24-52-24.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-100-0-0\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-100-0-0\\train_data_0-100-0-0.csv\n",
      "\tReading validation data from: model\\model_0-100-0-0\\val_data_0-100-0-0.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 181.335.9 MB/s, size: 1566.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 6761 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 229.3it/s 1:47<0.0ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 10.5it/s 2:26<0.1s\n",
      "                   all      24568      29413       0.39     0.0685      0.224      0.131\n",
      "Speed: 0.4ms preprocess, 4.7ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val12\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.2238\n",
      "mAP50-95: 0.1314\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 152.7786\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-100-0-0\\results\\results_0-100-0-0.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_27-73-0-0\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_27-73-0-0\\train_data_27-73-0-0.csv\n",
      "\tReading validation data from: model\\model_27-73-0-0\\val_data_27-73-0-0.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 114.232.9 MB/s, size: 1386.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 8981 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 254.6it/s 1:37<0.0ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 10.9it/s 2:21<0.1s\n",
      "                   all      24568      24187      0.583      0.145      0.374      0.248\n",
      "Speed: 0.4ms preprocess, 4.5ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val13\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3741\n",
      "mAP50-95: 0.2478\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 123.9092\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_27-73-0-0\\results\\results_27-73-0-0.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-0-30-70\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-0-30-70\\train_data_0-0-30-70.csv\n",
      "\tReading validation data from: model\\model_0-0-30-70\\val_data_0-0-30-70.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 145.374.1 MB/s, size: 649.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 8273 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 473.0it/s 51.9s<0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 12.0it/s 2:08<0.2s\n",
      "                   all      24568      24371      0.577      0.154       0.37      0.201\n",
      "Speed: 0.4ms preprocess, 4.0ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val14\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3702\n",
      "mAP50-95: 0.2012\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 127.6805\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-0-30-70\\results\\results_0-0-30-70.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_28-0-36-35\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_28-0-36-35\\train_data_28-0-36-35.csv\n",
      "\tReading validation data from: model\\model_28-0-36-35\\val_data_28-0-36-35.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 2.94.1 ms, read: 111.832.0 MB/s, size: 1360.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24567 images, 10040 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24567/24567 356.4it/s 1:09<0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.7it/s 2:11<0.2s\n",
      "                   all      24567      20800       0.69      0.243       0.48      0.303\n",
      "Speed: 0.3ms preprocess, 4.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val15\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.4801\n",
      "mAP50-95: 0.3031\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24567 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24567, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 107.6810\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_28-0-36-35\\results\\results_28-0-36-35.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_70-30-0-0\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_70-30-0-0\\train_data_70-30-0-0.csv\n",
      "\tReading validation data from: model\\model_70-30-0-0\\val_data_70-30-0-0.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.2 ms, read: 255.2175.6 MB/s, size: 1241.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 12783 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 310.0it/s 1:190.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.3it/s 2:16<0.1s\n",
      "                   all      24568      15260      0.794       0.38      0.611      0.434\n",
      "Speed: 0.3ms preprocess, 4.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val16\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.6111\n",
      "mAP50-95: 0.4341\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 75.0678\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_70-30-0-0\\results\\results_70-30-0-0.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_72-0-0-28\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_72-0-0-28\\train_data_72-0-0-28.csv\n",
      "\tReading validation data from: model\\model_72-0-0-28\\val_data_72-0-0-28.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 105.777.9 MB/s, size: 1738.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 13328 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 433.4it/s 56.7s0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 12.2it/s 2:05<0.2s\n",
      "                   all      24568      13248      0.824      0.486      0.681      0.469\n",
      "Speed: 0.3ms preprocess, 3.8ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val17\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.6814\n",
      "mAP50-95: 0.4693\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 62.6203\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_72-0-0-28\\results\\results_72-0-0-28.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-0-100-0\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-0-100-0\\train_data_0-0-100-0.csv\n",
      "\tReading validation data from: model\\model_0-0-100-0\\val_data_0-0-100-0.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 218.4122.3 MB/s, size: 1558.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 7362 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 229.5it/s 1:47<0.4ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 10.3it/s 2:29<0.2s\n",
      "                   all      24568      27600      0.592      0.165      0.378      0.228\n",
      "Speed: 0.4ms preprocess, 4.7ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val18\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3784\n",
      "mAP50-95: 0.2284\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 150.5490\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-0-100-0\\results\\results_0-0-100-0.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_26-35-0-39\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_26-35-0-39\\train_data_26-35-0-39.csv\n",
      "\tReading validation data from: model\\model_26-35-0-39\\val_data_26-35-0-39.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 162.7122.2 MB/s, size: 1100.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 9667 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 353.8it/s 1:09<0.0ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.8it/s 2:10<0.1s\n",
      "                   all      24568      21703      0.642      0.188      0.427      0.269\n",
      "Speed: 0.3ms preprocess, 4.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val19\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.4274\n",
      "mAP50-95: 0.2692\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 112.4845\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_26-35-0-39\\results\\results_26-35-0-39.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-27-0-73\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-27-0-73\\train_data_0-27-0-73.csv\n",
      "\tReading validation data from: model\\model_0-27-0-73\\val_data_0-27-0-73.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 134.8125.1 MB/s, size: 668.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 8140 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 478.4it/s 51.4s<0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 12.0it/s 2:08<0.2s\n",
      "                   all      24568      24668      0.533      0.128      0.335      0.177\n",
      "Speed: 0.3ms preprocess, 4.0ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val20\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3348\n",
      "mAP50-95: 0.1771\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 126.3170\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-27-0-73\\results\\results_0-27-0-73.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-70-30-0\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-70-30-0\\train_data_0-70-30-0.csv\n",
      "\tReading validation data from: model\\model_0-70-30-0\\val_data_0-70-30-0.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 184.324.9 MB/s, size: 1719.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 6937 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 227.9it/s 1:48<0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 10.3it/s 2:29<0.1s\n",
      "                   all      24568      28795      0.466     0.0973      0.279      0.166\n",
      "Speed: 0.4ms preprocess, 4.8ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val21\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.2792\n",
      "mAP50-95: 0.1664\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 152.9608\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-70-30-0\\results\\results_0-70-30-0.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_40-30-0-30\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_40-30-0-30\\train_data_40-30-0-30.csv\n",
      "\tReading validation data from: model\\model_40-30-0-30\\val_data_40-30-0-30.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 183.6112.0 MB/s, size: 1380.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 10738 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 346.4it/s 1:11<0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.6it/s 2:13<0.2s\n",
      "                   all      24568      19322      0.688      0.237       0.48      0.318\n",
      "Speed: 0.3ms preprocess, 4.1ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val22\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.4802\n",
      "mAP50-95: 0.3178\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 100.1989\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_40-30-0-30\\results\\results_40-30-0-30.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_27-0-73-0\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_27-0-73-0\\train_data_27-0-73-0.csv\n",
      "\tReading validation data from: model\\model_27-0-73-0\\val_data_27-0-73-0.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 4.23.7 ms, read: 122.262.1 MB/s, size: 2167.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 9450 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 254.3it/s 1:37<0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 10.7it/s 2:24<0.1s\n",
      "                   all      24568      22809      0.677      0.235      0.467      0.303\n",
      "Speed: 0.4ms preprocess, 4.5ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val23\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.4669\n",
      "mAP50-95: 0.3031\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 121.2880\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_27-0-73-0\\results\\results_27-0-73-0.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_70-0-30-0\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_70-0-30-0\\train_data_70-0-30-0.csv\n",
      "\tReading validation data from: model\\model_70-0-30-0\\val_data_70-0-30-0.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 169.063.0 MB/s, size: 1582.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 12858 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 306.9it/s 1:200.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.4it/s 2:15<0.2s\n",
      "                   all      24568      14854      0.808      0.441      0.648      0.456\n",
      "Speed: 0.3ms preprocess, 4.0ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val24\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.6484\n",
      "mAP50-95: 0.4556\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 72.5567\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_70-0-30-0\\results\\results_70-0-30-0.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-0-0-100\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-0-0-100\\train_data_0-0-0-100.csv\n",
      "\tReading validation data from: model\\model_0-0-0-100\\val_data_0-0-0-100.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 105.869.3 MB/s, size: 549.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 8660 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 766.4it/s 32.1s<0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 12.8it/s 2:00<0.2s\n",
      "                   all      24568      22918      0.584      0.156      0.378      0.195\n",
      "Speed: 0.3ms preprocess, 3.7ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val25\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3783\n",
      "mAP50-95: 0.1947\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 116.0662\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-0-0-100\\results\\results_0-0-0-100.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-26-27-47\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-26-27-47\\train_data_0-26-27-47.csv\n",
      "\tReading validation data from: model\\model_0-26-27-47\\val_data_0-26-27-47.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 1.12.4 ms, read: 84.449.3 MB/s, size: 1096.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 7767 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 334.6it/s 1:13<0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.2it/s 2:17<0.2s\n",
      "                   all      24568      25944      0.538      0.131      0.337      0.188\n",
      "Speed: 0.4ms preprocess, 4.3ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val26\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3367\n",
      "mAP50-95: 0.1877\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 139.3500\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-26-27-47\\results\\results_0-26-27-47.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-72-0-28\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-72-0-28\\train_data_0-72-0-28.csv\n",
      "\tReading validation data from: model\\model_0-72-0-28\\val_data_0-72-0-28.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 2.63.5 ms, read: 89.932.3 MB/s, size: 1419.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 7263 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 279.8it/s 1:28<0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.0it/s 2:20<0.2s\n",
      "                   all      24568      27623      0.446     0.0878      0.265      0.149\n",
      "Speed: 0.4ms preprocess, 4.5ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val27\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.2645\n",
      "mAP50-95: 0.1489\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 144.5631\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-72-0-28\\results\\results_0-72-0-28.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-0-50-50\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-0-50-50\\train_data_0-0-50-50.csv\n",
      "\tReading validation data from: model\\model_0-0-50-50\\val_data_0-0-50-50.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 152.227.3 MB/s, size: 847.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 8027 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 341.0it/s 1:12<0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.3it/s 2:16<0.2s\n",
      "                   all      24568      25226      0.575      0.157      0.369      0.209\n",
      "Speed: 0.4ms preprocess, 4.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val28\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3687\n",
      "mAP50-95: 0.2087\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 135.2463\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-0-50-50\\results\\results_0-0-50-50.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-50-0-50\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-50-0-50\\train_data_0-50-0-50.csv\n",
      "\tReading validation data from: model\\model_0-50-0-50\\val_data_0-50-0-50.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 190.1153.7 MB/s, size: 1537.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 7697 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 342.0it/s 1:12<0.0ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 11.4it/s 2:15<0.1s\n",
      "                   all      24568      26201      0.497      0.107      0.302      0.166\n",
      "Speed: 0.4ms preprocess, 4.2ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val29\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3022\n",
      "mAP50-95: 0.1658\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 136.9279\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-50-0-50\\results\\results_0-50-0-50.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-50-50-0\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-50-50-0\\train_data_0-50-50-0.csv\n",
      "\tReading validation data from: model\\model_0-50-50-0\\val_data_0-50-50-0.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 220.492.5 MB/s, size: 1823.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 7047 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 228.1it/s 1:48<0.0ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 10.2it/s 2:30<0.2s\n",
      "                   all      24568      28519      0.513      0.116      0.313      0.188\n",
      "Speed: 0.4ms preprocess, 4.7ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val30\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3128\n",
      "mAP50-95: 0.1882\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24568, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 150.9013\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-50-50-0\\results\\results_0-50-50-0.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "Processing model_0-33-33-33\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_0-33-33-33\\train_data_0-33-33-33.csv\n",
      "\tReading validation data from: model\\model_0-33-33-33\\val_data_0-33-33-33.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 143.145.7 MB/s, size: 1502.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24321 images, 7499 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24321/24321 291.3it/s 1:23<0.1ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1521/1521 10.8it/s 2:20<0.2s\n",
      "                   all      24321      26465      0.539      0.127      0.334      0.188\n",
      "Speed: 0.4ms preprocess, 4.4ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val31\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.3340\n",
      "mAP50-95: 0.1882\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24321 test images...\n",
      "Stacking feature embeddings into a single tensor...\n",
      "Feature embeddings tensor shape: torch.Size([24321, 1920])\n",
      "Calculating total variance across feature embeddings...\n",
      "Total Variance (Diversity Metric): 142.7943\n",
      "Diversity evaluation complete.\n",
      "--------------------------------------------------\n",
      "Saving results...\n",
      "Ensuring file path exists: model\\model_0-33-33-33\\results\\results_0-33-33-33.csv\n",
      "Results saved.\n",
      "==================================================\n",
      "All models processed.\n",
      "Combined results saved.\n"
     ]
    }
   ],
   "source": [
    "# Create a combined results dataframe to store all results\n",
    "combined_results_df = pd.DataFrame(columns=[\"data_blend\", \"map50\", \"map50-95\", \"total_variance\"])\n",
    "\n",
    "# Loop through the different models\n",
    "for suffix in model_suffixes:\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Processing model_{suffix}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Cleanup staging directory\n",
    "    print(\"Cleaning up staging directory...\")\n",
    "    utils.files.cleanup_staging()\n",
    "    print(\"Cleaning complete.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create a results dataframe to store the results\n",
    "    metrics_df = pd.DataFrame(columns=[\"data_blend\", \"map50\", \"map50-95\", \"total_variance\"])\n",
    "    \n",
    "    # Move data to the staging directory\n",
    "    print(\"Copying data to staging directory...\")\n",
    "    print(f\"\\tReading training data from: {model_paths[suffix]['train_df']}\")\n",
    "    print(f\"\\tReading validation data from: {model_paths[suffix]['val_df']}\")\n",
    "    train_df = pd.read_csv(model_paths[suffix][\"train_df\"])\n",
    "    val_df = pd.read_csv(model_paths[suffix][\"val_df\"])\n",
    "    utils.files.copy_to_staging(train_df, stage=\"test\")\n",
    "    utils.files.copy_to_staging(val_df, stage=\"test\")\n",
    "    print(\"Data copy complete.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Do stuff\n",
    "    # Load the model\n",
    "    print(f\"Loading baseline model from: {baseline_model}...\")\n",
    "    model = YOLO(baseline_model)\n",
    "    print(\"Model loaded.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Evaluate the model on the data mix\n",
    "    # Assess the recognizability metric (mAP50 and mAP50-95)\n",
    "    print(\"Evaluating model recognizability...\")\n",
    "    print(\"Running data blend through baseline model for evaluation...\")\n",
    "    recog_results = model.val(data=yaml_path, split=\"test\")\n",
    "    model_map50 = recog_results.box.map50\n",
    "    model_map50_95 = recog_results.box.map\n",
    "    print(\"Evaluation complete.\")\n",
    "    print(f\"mAP50: {model_map50:.4f}\")\n",
    "    print(f\"mAP50-95: {model_map50_95:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Evaluate the diversity metric\n",
    "    print(\"Evaluating model diversity...\")\n",
    "    layer_indicies = [10, 14, 17] # Indices of layers to extract feature embeddings from\n",
    "    blend_images = utils.files.get_image_files(os.path.join(\"data\", \"staging\", \"test\"))\n",
    "    print(f\"Extracting feature embeddings from {len(blend_images)} test images...\")\n",
    "    div_results = []\n",
    "    for img in blend_images:\n",
    "        div_result = model.predict(img, embed=layer_indicies, verbose=False)\n",
    "        div_results.append(div_result[0])\n",
    "    print(\"Stacking feature embeddings into a single tensor...\")\n",
    "    div_tensor = torch.stack(div_results)\n",
    "    print(f\"Feature embeddings tensor shape: {div_tensor.shape}\")\n",
    "    print(\"Calculating total variance across feature embeddings...\")\n",
    "    cov_matrix = torch.cov(div_tensor.view(div_tensor.size(0), -1).T)\n",
    "    diversity_metric = torch.trace(cov_matrix).item()\n",
    "    print(f\"Total Variance (Diversity Metric): {diversity_metric:.4f}\")\n",
    "    print(\"Diversity evaluation complete.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Append results to the dataframe\n",
    "    print(\"Saving results...\")\n",
    "    print(f\"Ensuring file path exists: {model_paths[suffix]['results_df']}\")\n",
    "    os.makedirs(os.path.dirname(model_paths[suffix]['results_df']), exist_ok=True)\n",
    "    metrics = pd.Series({\n",
    "        \"data_blend\": suffix,\n",
    "        \"map50\": model_map50,\n",
    "        \"map50-95\": model_map50_95,\n",
    "        \"total_variance\": diversity_metric\n",
    "        })\n",
    "    metrics_df = pd.concat([metrics_df, metrics.to_frame().T], ignore_index=True)\n",
    "    combined_results_df = pd.concat([combined_results_df, metrics.to_frame().T], ignore_index=True)\n",
    "    # Save the results dataframe\n",
    "    metrics_df.to_csv(model_paths[suffix][\"results_df\"], index=False)\n",
    "    print(\"Results saved.\")\n",
    "    \n",
    "print(\"=\" * 50)\n",
    "print(\"All models processed.\")\n",
    "\n",
    "# Save the combined results dataframe\n",
    "combined_results_path = os.path.join(\"reports\", \"diversity_recognizability_results.csv\")\n",
    "combined_results_df.to_csv(combined_results_path, index=False)\n",
    "print(\"Combined results saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ebb13a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sydrone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
