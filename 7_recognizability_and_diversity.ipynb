{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7111cc1d",
   "metadata": {},
   "source": [
    "# ðŸ”Ž Recognizability and ðŸŽ† Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bbbfc",
   "metadata": {},
   "source": [
    "The ideas of recognizability and diversity of a data set introducted by Boutin et al. (2022) are used to evaluate the ability of a generative model to create useful data. The recognizability metric is easiest to understand as simply how easy (or difficult) it is for the data to be classified. Therefore, in the case of the drone data it is just a measure of how easily the drone objects can be identified within the images. Diversity is best thought of as the variance of the feature space of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a7a56",
   "metadata": {},
   "source": [
    "`Boutin, V., Singhal, L., Thomas, X., & Serre, T. (2022). Diversity vs. Recognizability: Human-like generalization in one-shot generative models. Advances in Neural Information Processing Systems, 35, 20933-20946.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab3ea6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901175c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63805d88",
   "metadata": {},
   "source": [
    "## 1. File Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba3641",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the staging directory if it doesn't exist\n",
    "os.makedirs(os.path.join(\"data\", \"staging\"), exist_ok=True)\n",
    "# Create the test folder in the staging directory if it doesn't exist\n",
    "os.makedirs(os.path.join(\"data\", \"staging\", \"test\"), exist_ok=True)\n",
    "# Create train and val folders (they won't be used but need to exist for the YOLO model functions)\n",
    "os.makedirs(os.path.join(\"data\", \"staging\", \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(\"data\", \"staging\", \"val\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5551278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the YOLO yaml file\n",
    "yaml_path = os.path.join(\"data\", \"staging\", \"evaluation.yaml\")\n",
    "\n",
    "# YAML content\n",
    "yaml_content = \"\"\"\n",
    "path: data/staging  # dataset root dir (leave empty for HUB)\n",
    "train: train  # train images (relative to 'path')\n",
    "val:   val    # val images (relative to 'path')\n",
    "test:  test   # test images (relative to 'path')\n",
    "\n",
    "names:\n",
    "  0: drone\n",
    "\"\"\"\n",
    "\n",
    "# Write the YAML content to the file\n",
    "with open(yaml_path, 'w') as yaml_file:\n",
    "    yaml_file.write(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab8af6",
   "metadata": {},
   "source": [
    "**Experimental Design**\n",
    "\n",
    "| **Run** | **Authentic Data** | **3D Model Data** | **Clipart Data** | **Gen AI Data** |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 00  | 100 | 0   | 0   | 0   |\n",
    "| 01  | 26  | 27  | 23  | 24  |\n",
    "| 02  | 0   | 57  | 22  | 21  |\n",
    "| 03  | 0   | 27  | 73  | 0   |\n",
    "| 04  | 27  | 0   | 0   | 73  |\n",
    "| 05  | 27  | 33  | 40  | 0   |\n",
    "| 06  | 38  | 0   | 33  | 29  |\n",
    "| 07  | 38  | 33  | 29  | 0   |\n",
    "| 08  | 0   | 0   | 76  | 24  |\n",
    "| 09  | 0   | 24  | 52  | 24  |\n",
    "| 10  | 0   | 100 | 0   | 0   |\n",
    "| 11  | 27  | 73  | 0   | 0   |\n",
    "| 12  | 0   | 0   | 30  | 70  |\n",
    "| 13  | 29  | 0   | 36  | 35  |\n",
    "| 14  | 70  | 30  | 0   | 0   |\n",
    "| 15  | 72  | 0   | 0   | 28  |\n",
    "| 16  | 0   | 0   | 100 | 0   |\n",
    "| 17  | 26  | 35  | 0   | 39  |\n",
    "| 18  | 0   | 27  | 0   | 73  |\n",
    "| 19  | 0   | 70  | 30  | 0   |\n",
    "| 20  | 40  | 30  | 0   | 30  |\n",
    "| 21  | 27  | 0   | 73  | 0   |\n",
    "| 22  | 70  | 0   | 30  | 0   |\n",
    "| 23  | 0   | 0   | 0   | 100 |\n",
    "| 24  | 0   | 26  | 27  | 47  |\n",
    "| 25  | 0   | 72  | 0   | 28  |\n",
    "| 26* | 0   | 0   | 50  | 50  |\n",
    "| 27* | 0   | 50  | 0   | 50  |\n",
    "| 28* | 0   | 50  | 50  | 0   |\n",
    "| 29* | 0   | 33  | 33  | 33  |\n",
    "\n",
    "`* denotes extra runs that were added`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af19956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model suffixes\n",
    "# It would be better to do this by searching the directory, but this is quicker for now.\n",
    "model_00 = \"baseline\"\n",
    "\n",
    "model_01 = \"26-27-23-24\"\n",
    "model_02 = \"0-56-22-21\"\n",
    "model_03 = \"0-27-73-0\"\n",
    "model_04 = \"27-0-0-73\"\n",
    "model_05 = \"27-33-40-0\"\n",
    "model_06 = \"38-0-33-28\"\n",
    "model_07 = \"38-33-28-0\"\n",
    "model_08 = \"0-0-76-24\"\n",
    "model_09 = \"0-24-52-24\"\n",
    "model_10 = \"0-100-0-0\"\n",
    "model_11 = \"27-73-0-0\"\n",
    "model_12 = \"0-0-30-70\"\n",
    "model_13 = \"28-0-36-35\"\n",
    "model_14 = \"70-30-0-0\"\n",
    "model_15 = \"72-0-0-28\"\n",
    "model_16 = \"0-0-100-0\"\n",
    "model_17 = \"26-35-0-39\"\n",
    "model_18 = \"0-27-0-73\"\n",
    "model_19 = \"0-70-30-0\"\n",
    "model_20 = \"40-30-0-30\"\n",
    "model_21 = \"27-0-73-0\"\n",
    "model_22 = \"70-0-30-0\"\n",
    "model_23 = \"0-0-0-100\"\n",
    "model_24 = \"0-26-27-47\"\n",
    "model_25 = \"0-72-0-28\"\n",
    "model_26 = \"0-0-50-50\"\n",
    "model_27 = \"0-50-0-50\"\n",
    "model_28 = \"0-50-50-0\"\n",
    "model_29 = \"0-33-33-33\"\n",
    "\n",
    "# Combine all model suffixes into a list\n",
    "model_suffixes = [model_00,\n",
    "                  model_01,\n",
    "                  model_02,\n",
    "                  model_03,\n",
    "                  model_04,\n",
    "                  model_05,\n",
    "                  model_06,\n",
    "                  model_07,\n",
    "                  model_08,\n",
    "                  model_09,\n",
    "                  model_10,\n",
    "                  model_11,\n",
    "                  model_12,\n",
    "                  model_13,\n",
    "                  model_14,\n",
    "                  model_15,\n",
    "                  model_16,\n",
    "                  model_17,\n",
    "                  model_18,\n",
    "                  model_19,\n",
    "                  model_20,\n",
    "                  model_21,\n",
    "                  model_22,\n",
    "                  model_23,\n",
    "                  model_24,\n",
    "                  model_25,\n",
    "                  model_26,\n",
    "                  model_27,\n",
    "                  model_28,\n",
    "                  model_29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4826e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "for suffix in model_suffixes:\n",
    "    dir_path = os.path.join(\"model\", f\"model-{suffix}\")\n",
    "    os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e11aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of important file paths for each model\n",
    "model_paths = {\n",
    "    suffix: {\n",
    "        \"weights\": os.path.join(\"model\", f\"model_{suffix}\", \"weights\", \"best.pt\"),\n",
    "        \"train_df\": os.path.join(\"model\", f\"model_{suffix}\", f\"train_data_{suffix}.csv\"),\n",
    "        \"val_df\": os.path.join(\"model\", f\"model_{suffix}\", f\"val_data_{suffix}.csv\"),\n",
    "        \"results_df\": os.path.join(\"model\", f\"model_{suffix}\", \"results\", f\"results_{suffix}.csv\"),\n",
    "        }\n",
    "    for suffix in model_suffixes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e500c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights file for the baseline model for comparison purposes\n",
    "baseline_model = model_paths[model_00][\"weights\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5e0ee",
   "metadata": {},
   "source": [
    "## 2. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f888fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97fa88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined results dataframe to store all results\n",
    "combined_results_df = pd.DataFrame(columns=[\"data_blend\", \"map50\", \"map50-95\", \"total_variance\"])\n",
    "\n",
    "# Loop through the different models\n",
    "for suffix in model_suffixes:\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Processing model_{suffix}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Cleanup staging directory\n",
    "    print(\"Cleaning up staging directory...\")\n",
    "    utils.files.cleanup_staging()\n",
    "    print(\"Cleaning complete.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create a results dataframe to store the results\n",
    "    metrics_df = pd.DataFrame(columns=[\"data_blend\", \"map50\", \"map50-95\", \"total_variance\"])\n",
    "    \n",
    "    # Move data to the staging directory\n",
    "    print(\"Copying data to staging directory...\")\n",
    "    print(f\"\\tReading training data from: {model_paths[suffix]['train_df']}\")\n",
    "    print(f\"\\tReading validation data from: {model_paths[suffix]['val_df']}\")\n",
    "    train_df = pd.read_csv(model_paths[suffix][\"train_df\"])\n",
    "    val_df = pd.read_csv(model_paths[suffix][\"val_df\"])\n",
    "    utils.files.copy_to_staging(train_df, stage=\"test\")\n",
    "    utils.files.copy_to_staging(val_df, stage=\"test\")\n",
    "    print(\"Data copy complete.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Do stuff\n",
    "    # Load the model\n",
    "    print(f\"Loading baseline model from: {baseline_model}...\")\n",
    "    model = YOLO(baseline_model)\n",
    "    print(\"Model loaded.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Evaluate the model on the data mix\n",
    "    # Assess the recognizability metric (mAP50 and mAP50-95)\n",
    "    print(\"Evaluating model recognizability...\")\n",
    "    print(\"Running data blend through baseline model for evaluation...\")\n",
    "    recog_results = model.val(data=yaml_path, split=\"test\")\n",
    "    model_map50 = recog_results.box.map50\n",
    "    model_map50_95 = recog_results.box.map\n",
    "    print(\"Evaluation complete.\")\n",
    "    print(f\"mAP50: {model_map50:.4f}\")\n",
    "    print(f\"mAP50-95: {model_map50_95:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Evaluate the diversity metric\n",
    "    print(\"Evaluating model diversity...\")\n",
    "    layer_indicies = [10, 14, 17] # Indices of layers to extract feature embeddings from\n",
    "    blend_images = utils.files.get_image_files(os.path.join(\"data\", \"staging\", \"test\"))\n",
    "    print(f\"Extracting feature embeddings from {len(blend_images)} test images...\")\n",
    "    div_results = model.predict(blend_images, embed=layer_indicies)\n",
    "    print(\"Stacking feature embeddings into a single tensor...\")\n",
    "    div_tensor = torch.stack(div_results)\n",
    "    print(f\"Feature embeddings tensor shape: {div_tensor.shape}\")\n",
    "    print(\"Calculating total variance across feature embeddings...\")\n",
    "    cov_matrix = torch.cov(div_tensor.view(div_tensor.size(0), -1).T)\n",
    "    diversity_metric = torch.trace(cov_matrix).item()\n",
    "    print(f\"Total Variance (Diversity Metric): {diversity_metric:.4f}\")\n",
    "    print(\"Diversity evaluation complete.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Append results to the dataframe\n",
    "    print(\"Saving results...\")\n",
    "    print(f\"Ensuring file path exists: {model_paths[suffix]['results_df']}\")\n",
    "    os.makedirs(os.path.dirname(model_paths[suffix]['results_df']), exist_ok=True)\n",
    "    metrics_df = metrics_df.append({\n",
    "        \"data_blend\": suffix,\n",
    "        \"map50\": model_map50,\n",
    "        \"map50-95\": model_map50_95,\n",
    "        \"total_variance\": diversity_metric\n",
    "    }, ignore_index=True)\n",
    "    combined_results_df = combined_results_df.append({\n",
    "        \"data_blend\": suffix,\n",
    "        \"map50\": model_map50,\n",
    "        \"map50-95\": model_map50_95,\n",
    "        \"total_variance\": diversity_metric\n",
    "    }, ignore_index=True)\n",
    "    # Save the results dataframe\n",
    "    metrics_df.to_csv(model_paths[suffix][\"results_df\"], index=False)\n",
    "    print(\"Results saved.\")\n",
    "    \n",
    "print(\"=\" * 50)\n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sydrone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
