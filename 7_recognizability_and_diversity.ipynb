{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7111cc1d",
   "metadata": {},
   "source": [
    "# üîé Recognizability and üéÜ Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5bbbfc",
   "metadata": {},
   "source": [
    "The ideas of recognizability and diversity of a data set introducted by Boutin et al. (2022) are used to evaluate the ability of a generative model to create useful data. The recognizability metric is easiest to understand as simply how easy (or difficult) it is for the data to be classified. Therefore, in the case of the drone data it is just a measure of how easily the drone objects can be identified within the images. Diversity is best thought of as the variance of the feature space of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a7a56",
   "metadata": {},
   "source": [
    "`Boutin, V., Singhal, L., Thomas, X., & Serre, T. (2022). Diversity vs. Recognizability: Human-like generalization in one-shot generative models. Advances in Neural Information Processing Systems, 35, 20933-20946.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab3ea6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "901175c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from src import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63805d88",
   "metadata": {},
   "source": [
    "## 1. File Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba3641",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb14c44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the staging directory if it doesn't exist\n",
    "os.makedirs(os.path.join(\"data\", \"staging\"), exist_ok=True)\n",
    "# Create the test folder in the staging directory if it doesn't exist\n",
    "os.makedirs(os.path.join(\"data\", \"staging\", \"test\"), exist_ok=True)\n",
    "# Create train and val folders (they won't be used but need to exist for the YOLO model functions)\n",
    "os.makedirs(os.path.join(\"data\", \"staging\", \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(\"data\", \"staging\", \"val\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5551278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the YOLO yaml file\n",
    "yaml_path = os.path.join(\"data\", \"staging\", \"evaluation.yaml\")\n",
    "\n",
    "# YAML content\n",
    "yaml_content = \"\"\"\n",
    "path: data/staging  # dataset root dir (leave empty for HUB)\n",
    "train: train  # train images (relative to 'path')\n",
    "val:   val    # val images (relative to 'path')\n",
    "test:  test   # test images (relative to 'path')\n",
    "\n",
    "names:\n",
    "  0: drone\n",
    "\"\"\"\n",
    "\n",
    "# Write the YAML content to the file\n",
    "with open(yaml_path, 'w') as yaml_file:\n",
    "    yaml_file.write(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab8af6",
   "metadata": {},
   "source": [
    "**Experimental Design**\n",
    "\n",
    "| **Run** | **Authentic Data** | **3D Model Data** | **Clipart Data** | **Gen AI Data** |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 00  | 100 | 0   | 0   | 0   |\n",
    "| 01  | 26  | 27  | 23  | 24  |\n",
    "| 02  | 0   | 57  | 22  | 21  |\n",
    "| 03  | 0   | 27  | 73  | 0   |\n",
    "| 04  | 27  | 0   | 0   | 73  |\n",
    "| 05  | 27  | 33  | 40  | 0   |\n",
    "| 06  | 38  | 0   | 33  | 29  |\n",
    "| 07  | 38  | 33  | 29  | 0   |\n",
    "| 08  | 0   | 0   | 76  | 24  |\n",
    "| 09  | 0   | 24  | 52  | 24  |\n",
    "| 10  | 0   | 100 | 0   | 0   |\n",
    "| 11  | 27  | 73  | 0   | 0   |\n",
    "| 12  | 0   | 0   | 30  | 70  |\n",
    "| 13  | 29  | 0   | 36  | 35  |\n",
    "| 14  | 70  | 30  | 0   | 0   |\n",
    "| 15  | 72  | 0   | 0   | 28  |\n",
    "| 16  | 0   | 0   | 100 | 0   |\n",
    "| 17  | 26  | 35  | 0   | 39  |\n",
    "| 18  | 0   | 27  | 0   | 73  |\n",
    "| 19  | 0   | 70  | 30  | 0   |\n",
    "| 20  | 40  | 30  | 0   | 30  |\n",
    "| 21  | 27  | 0   | 73  | 0   |\n",
    "| 22  | 70  | 0   | 30  | 0   |\n",
    "| 23  | 0   | 0   | 0   | 100 |\n",
    "| 24  | 0   | 26  | 27  | 47  |\n",
    "| 25  | 0   | 72  | 0   | 28  |\n",
    "| 26* | 0   | 0   | 50  | 50  |\n",
    "| 27* | 0   | 50  | 0   | 50  |\n",
    "| 28* | 0   | 50  | 50  | 0   |\n",
    "| 29* | 0   | 33  | 33  | 33  |\n",
    "\n",
    "`* denotes extra runs that were added`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af19956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model suffixes\n",
    "# It would be better to do this by searching the directory, but this is quicker for now.\n",
    "model_00 = \"baseline\"\n",
    "\n",
    "model_01 = \"26-27-23-24\"\n",
    "model_02 = \"0-56-22-21\"\n",
    "model_03 = \"0-27-73-0\"\n",
    "model_04 = \"27-0-0-73\"\n",
    "model_05 = \"27-33-40-0\"\n",
    "model_06 = \"38-0-33-28\"\n",
    "model_07 = \"38-33-28-0\"\n",
    "model_08 = \"0-0-76-24\"\n",
    "model_09 = \"0-24-52-24\"\n",
    "model_10 = \"0-100-0-0\"\n",
    "model_11 = \"27-73-0-0\"\n",
    "model_12 = \"0-0-30-70\"\n",
    "model_13 = \"28-0-36-35\"\n",
    "model_14 = \"70-30-0-0\"\n",
    "model_15 = \"72-0-0-28\"\n",
    "model_16 = \"0-0-100-0\"\n",
    "model_17 = \"26-35-0-39\"\n",
    "model_18 = \"0-27-0-73\"\n",
    "model_19 = \"0-70-30-0\"\n",
    "model_20 = \"40-30-0-30\"\n",
    "model_21 = \"27-0-73-0\"\n",
    "model_22 = \"70-0-30-0\"\n",
    "model_23 = \"0-0-0-100\"\n",
    "model_24 = \"0-26-27-47\"\n",
    "model_25 = \"0-72-0-28\"\n",
    "model_26 = \"0-0-50-50\"\n",
    "model_27 = \"0-50-0-50\"\n",
    "model_28 = \"0-50-50-0\"\n",
    "model_29 = \"0-33-33-33\"\n",
    "\n",
    "# Combine all model suffixes into a list\n",
    "model_suffixes = [model_00,\n",
    "                  model_01,\n",
    "                  model_02,\n",
    "                  model_03,\n",
    "                  model_04,\n",
    "                  model_05,\n",
    "                  model_06,\n",
    "                  model_07,\n",
    "                  model_08,\n",
    "                  model_09,\n",
    "                  model_10,\n",
    "                  model_11,\n",
    "                  model_12,\n",
    "                  model_13,\n",
    "                  model_14,\n",
    "                  model_15,\n",
    "                  model_16,\n",
    "                  model_17,\n",
    "                  model_18,\n",
    "                  model_19,\n",
    "                  model_20,\n",
    "                  model_21,\n",
    "                  model_22,\n",
    "                  model_23,\n",
    "                  model_24,\n",
    "                  model_25,\n",
    "                  model_26,\n",
    "                  model_27,\n",
    "                  model_28,\n",
    "                  model_29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a4826e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories if they don't exist\n",
    "for suffix in model_suffixes:\n",
    "    dir_path = os.path.join(\"model\", f\"model_{suffix}\")\n",
    "    os.makedirs(dir_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1e11aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of important file paths for each model\n",
    "model_paths = {\n",
    "    suffix: {\n",
    "        \"weights\": os.path.join(\"model\", f\"model_{suffix}\", \"weights\", \"best.pt\"),\n",
    "        \"train_df\": os.path.join(\"model\", f\"model_{suffix}\", f\"train_data_{suffix}.csv\"),\n",
    "        \"val_df\": os.path.join(\"model\", f\"model_{suffix}\", f\"val_data_{suffix}.csv\"),\n",
    "        \"results_df\": os.path.join(\"model\", f\"model_{suffix}\", \"results\", f\"results_{suffix}.csv\"),\n",
    "        }\n",
    "    for suffix in model_suffixes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e500c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights file for the baseline model for comparison purposes\n",
    "baseline_model = model_paths[model_00][\"weights\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b5e0ee",
   "metadata": {},
   "source": [
    "## 2. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f888fd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97fa88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Processing model_baseline\n",
      "--------------------------------------------------\n",
      "Cleaning up staging directory...\n",
      "Cleaning complete.\n",
      "--------------------------------------------------\n",
      "Copying data to staging directory...\n",
      "\tReading training data from: model\\model_baseline\\train_data_baseline.csv\n",
      "\tReading validation data from: model\\model_baseline\\val_data_baseline.csv\n",
      "Data copy complete.\n",
      "--------------------------------------------------\n",
      "Loading baseline model from: model\\model_baseline\\weights\\best.pt...\n",
      "Model loaded.\n",
      "--------------------------------------------------\n",
      "Evaluating model recognizability...\n",
      "Running data blend through baseline model for evaluation...\n",
      "Ultralytics 8.3.221  Python-3.13.5 torch-2.7.1+cu128 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11x summary (fused): 190 layers, 56,828,179 parameters, 0 gradients, 194.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 196.1197.5 MB/s, size: 1140.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test... 24568 images, 15263 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24568/24568 496.5it/s 49.5s0.0ss\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: E:\\Python\\synthetic-data-for-object-detection\\data\\staging\\test.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1536/1536 12.0it/s 2:08<0.1s\n",
      "                   all      24568       9327      0.894      0.797      0.873      0.634\n",
      "Speed: 0.3ms preprocess, 4.0ms inference, 0.0ms loss, 0.2ms postprocess per image\n",
      "Results saved to \u001b[1mE:\\Python\\synthetic-data-for-object-detection\\runs\\detect\\val8\u001b[0m\n",
      "Evaluation complete.\n",
      "mAP50: 0.8732\n",
      "mAP50-95: 0.6341\n",
      "--------------------------------------------------\n",
      "Evaluating model diversity...\n",
      "Extracting feature embeddings from 24568 test images...\n",
      "\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3667, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "    ~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\keyan\\AppData\\Local\\Temp\\ipykernel_80572\\142844981.py\", line 55, in <module>\n",
      "    div_results = model.predict(blend_images, embed=layer_indicies)\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\ultralytics\\engine\\model.py\", line 557, in predict\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 230, in __call__\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 36, in generator_context\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 307, in stream_inference\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\ultralytics\\engine\\predictor.py\", line 262, in setup_source\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\ultralytics\\data\\build.py\", line 409, in load_inference_source\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\ultralytics\\data\\build.py\", line 376, in check_source\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\ultralytics\\data\\loaders.py\", line 646, in autocast_list\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\PIL\\Image.py\", line 3465, in open\n",
      "OSError: [Errno 24] Too many open files: 'data\\\\staging\\\\test\\\\dji_matrice_210_mountain_frame_1360_00.png'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\executing\\executing.py\", line 317, in executing\n",
      "KeyError: (<code object run_code at 0x000001FBD5B04C50, file \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3631>, 2181133519952, 256)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2176, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "        etype, value, tb, tb_offset=tb_offset\n",
      "    )\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1182, in structured_traceback\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1053, in structured_traceback\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 861, in structured_traceback\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 746, in format_exception_as_a_whole\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 848, in get_records\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\stack_data\\core.py\", line 565, in stack_data\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\stack_data\\utils.py\", line 84, in collapse_repeated\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\stack_data\\core.py\", line 555, in mapper\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\stack_data\\core.py\", line 520, in __init__\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\executing\\executing.py\", line 369, in executing\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\executing\\executing.py\", line 252, in for_frame\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\executing\\executing.py\", line 270, in for_filename\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\executing\\executing.py\", line 281, in _for_filename_and_lines\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\stack_data\\core.py\", line 81, in __init__\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\executing\\executing.py\", line 413, in asttokens\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\asttokens\\asttokens.py\", line 120, in __init__\n",
      "  File \"c:\\ProgramData\\anaconda3\\envs\\sydrone\\Lib\\site-packages\\asttokens\\asttokens.py\", line 131, in mark_tokens\n",
      "  File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1022, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1159, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1217, in get_data\n",
      "OSError: [Errno 24] Too many open files: 'c:\\\\ProgramData\\\\anaconda3\\\\envs\\\\sydrone\\\\Lib\\\\site-packages\\\\asttokens\\\\mark_tokens.py'\n"
     ]
    }
   ],
   "source": [
    "# Create a combined results dataframe to store all results\n",
    "combined_results_df = pd.DataFrame(columns=[\"data_blend\", \"map50\", \"map50-95\", \"total_variance\"])\n",
    "\n",
    "# Loop through the different models\n",
    "for suffix in model_suffixes:\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Processing model_{suffix}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Cleanup staging directory\n",
    "    print(\"Cleaning up staging directory...\")\n",
    "    utils.files.cleanup_staging()\n",
    "    print(\"Cleaning complete.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create a results dataframe to store the results\n",
    "    metrics_df = pd.DataFrame(columns=[\"data_blend\", \"map50\", \"map50-95\", \"total_variance\"])\n",
    "    \n",
    "    # Move data to the staging directory\n",
    "    print(\"Copying data to staging directory...\")\n",
    "    print(f\"\\tReading training data from: {model_paths[suffix]['train_df']}\")\n",
    "    print(f\"\\tReading validation data from: {model_paths[suffix]['val_df']}\")\n",
    "    train_df = pd.read_csv(model_paths[suffix][\"train_df\"])\n",
    "    val_df = pd.read_csv(model_paths[suffix][\"val_df\"])\n",
    "    utils.files.copy_to_staging(train_df, stage=\"test\")\n",
    "    utils.files.copy_to_staging(val_df, stage=\"test\")\n",
    "    print(\"Data copy complete.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Do stuff\n",
    "    # Load the model\n",
    "    print(f\"Loading baseline model from: {baseline_model}...\")\n",
    "    model = YOLO(baseline_model)\n",
    "    print(\"Model loaded.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Evaluate the model on the data mix\n",
    "    # Assess the recognizability metric (mAP50 and mAP50-95)\n",
    "    print(\"Evaluating model recognizability...\")\n",
    "    print(\"Running data blend through baseline model for evaluation...\")\n",
    "    recog_results = model.val(data=yaml_path, split=\"test\")\n",
    "    model_map50 = recog_results.box.map50\n",
    "    model_map50_95 = recog_results.box.map\n",
    "    print(\"Evaluation complete.\")\n",
    "    print(f\"mAP50: {model_map50:.4f}\")\n",
    "    print(f\"mAP50-95: {model_map50_95:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Evaluate the diversity metric\n",
    "    print(\"Evaluating model diversity...\")\n",
    "    layer_indicies = [10, 14, 17] # Indices of layers to extract feature embeddings from\n",
    "    blend_images = utils.files.get_image_files(os.path.join(\"data\", \"staging\", \"test\"))\n",
    "    print(f\"Extracting feature embeddings from {len(blend_images)} test images...\")\n",
    "    div_results = []\n",
    "    for img in blend_images:\n",
    "        div_result = model.predict(img, embed=layer_indicies)\n",
    "        div_results.append(div_result)\n",
    "    print(\"Stacking feature embeddings into a single tensor...\")\n",
    "    div_tensor = torch.stack(div_results)\n",
    "    print(f\"Feature embeddings tensor shape: {div_tensor.shape}\")\n",
    "    print(\"Calculating total variance across feature embeddings...\")\n",
    "    cov_matrix = torch.cov(div_tensor.view(div_tensor.size(0), -1).T)\n",
    "    diversity_metric = torch.trace(cov_matrix).item()\n",
    "    print(f\"Total Variance (Diversity Metric): {diversity_metric:.4f}\")\n",
    "    print(\"Diversity evaluation complete.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Append results to the dataframe\n",
    "    print(\"Saving results...\")\n",
    "    print(f\"Ensuring file path exists: {model_paths[suffix]['results_df']}\")\n",
    "    os.makedirs(os.path.dirname(model_paths[suffix]['results_df']), exist_ok=True)\n",
    "    metrics_df = metrics_df.append({\n",
    "        \"data_blend\": suffix,\n",
    "        \"map50\": model_map50,\n",
    "        \"map50-95\": model_map50_95,\n",
    "        \"total_variance\": diversity_metric\n",
    "    }, ignore_index=True)\n",
    "    combined_results_df = combined_results_df.append({\n",
    "        \"data_blend\": suffix,\n",
    "        \"map50\": model_map50,\n",
    "        \"map50-95\": model_map50_95,\n",
    "        \"total_variance\": diversity_metric\n",
    "    }, ignore_index=True)\n",
    "    # Save the results dataframe\n",
    "    metrics_df.to_csv(model_paths[suffix][\"results_df\"], index=False)\n",
    "    print(\"Results saved.\")\n",
    "    \n",
    "print(\"=\" * 50)\n",
    "print(\"All models processed.\")\n",
    "\n",
    "# Save the combined results dataframe\n",
    "combined_results_path = os.path.join(\"reports\", \"diversity_recognizability_results.csv\")\n",
    "combined_results_df.to_csv(combined_results_path, index=False)\n",
    "print(\"Combined results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sydrone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
